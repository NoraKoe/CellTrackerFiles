{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseDataGenerator(ImageDataGenerator):\n",
    "    def flow(self,\n",
    "             train_dict,\n",
    "             crop_dim=32,\n",
    "             min_track_length=5,\n",
    "             batch_size=32,\n",
    "             shuffle=True,\n",
    "             seed=None,\n",
    "             data_format=None,\n",
    "             save_to_dir=None,\n",
    "             save_prefix='',\n",
    "             save_format='png'):\n",
    "        return SiameseIterator(\n",
    "            train_dict,\n",
    "            self,\n",
    "            crop_dim=crop_dim,\n",
    "            min_track_length=min_track_length,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            seed=seed,\n",
    "            data_format=data_format,\n",
    "            save_to_dir=save_to_dir,\n",
    "            save_prefix=save_prefix,\n",
    "            save_format=save_format)\n",
    "\n",
    "\n",
    "class SiameseIterator(Iterator):\n",
    "    def __init__(self,\n",
    "                 train_dict,\n",
    "                 image_data_generator,\n",
    "                 crop_dim=14,\n",
    "                 min_track_length=5,\n",
    "                 batch_size=32,\n",
    "                 shuffle=False,\n",
    "                 seed=None,\n",
    "                 squeeze=False,\n",
    "                 data_format=None,\n",
    "                 save_to_dir=None,\n",
    "                 save_prefix='',\n",
    "                 save_format='png'):\n",
    "        if data_format is None:\n",
    "            data_format = K.image_data_format()\n",
    "\n",
    "        if data_format == 'channels_first':\n",
    "            self.channel_axis = 1\n",
    "            self.row_axis = 3\n",
    "            self.col_axis = 4\n",
    "            self.time_axis = 2\n",
    "        if data_format == 'channels_last':\n",
    "            self.channel_axis = 4\n",
    "            self.row_axis = 2\n",
    "            self.col_axis = 3\n",
    "            self.time_axis = 1\n",
    "        self.x = np.asarray(train_dict['X'], dtype=K.floatx())\n",
    "        self.y = np.array(train_dict['y'], dtype='int32')\n",
    "\n",
    "        if self.x.ndim != 5:\n",
    "            raise ValueError('Input data in `SiameseIterator` '\n",
    "                             'should have rank 5. You passed an array '\n",
    "                             'with shape', self.x.shape)\n",
    "\n",
    "        self.crop_dim = crop_dim\n",
    "        self.min_track_length = min_track_length\n",
    "        self.image_data_generator = image_data_generator\n",
    "        self.squeeze = squeeze\n",
    "        self.data_format = data_format\n",
    "        self.save_to_dir = save_to_dir\n",
    "        self.save_prefix = save_prefix\n",
    "        self.save_format = save_format\n",
    "\n",
    "        if 'daughters' in train_dict:\n",
    "            self.daughters = train_dict['daughters']\n",
    "        else:\n",
    "            self.daughters = None\n",
    "\n",
    "        self._remove_bad_images()\n",
    "        self._create_track_ids()\n",
    "        self._create_appearances()\n",
    "\n",
    "        super(SiameseIterator, self).__init__(\n",
    "            len(self.track_ids), batch_size, shuffle, seed)\n",
    "\n",
    "    def _remove_bad_images(self):\n",
    "        \"\"\"\n",
    "        This function goes through all of the batches of images and removes the \n",
    "        images that only have one cell.\n",
    "        This could be expanded to include questionable annotations\n",
    "        \"\"\"\n",
    "        good_batches = []\n",
    "        number_of_batches = self.x.shape[0]\n",
    "        for batch in range(number_of_batches):\n",
    "            y = self.y[batch]\n",
    "            unique_ids = np.unique(y.flatten())\n",
    "            if len(unique_ids) > 2: # You should have at least 3 id's - 2 cells and 1 background\n",
    "                good_batches.append(batch)\n",
    "\n",
    "        X_new_shape = tuple([len(good_batches)] + list(self.x.shape[1:]))\n",
    "        y_new_shape = tuple([len(good_batches)] + list(self.y.shape[1:]))\n",
    "\n",
    "        X_new = np.zeros(X_new_shape, dtype = K.floatx())\n",
    "        y_new = np.zeros(y_new_shape, dtype = np.int32)\n",
    "\n",
    "        counter = 0\n",
    "        for k, batch in enumerate(good_batches):\n",
    "            X_new[k] = self.x[batch]\n",
    "            y_new[k] = self.y[batch]\n",
    "\n",
    "        self.x = X_new\n",
    "        self.y = y_new\n",
    "\n",
    "        return None\n",
    "\n",
    "    def _create_track_ids(self):\n",
    "        \"\"\"\n",
    "        This function builds the track id's. It returns a dictionary that\n",
    "        contains the batch number and label number of each each track.\n",
    "        Creates unique cell IDs, as cell labels are NOT unique across batches.\n",
    "        \"\"\"\n",
    "        track_counter = 0\n",
    "        track_ids = {}\n",
    "        for batch in range(self.y.shape[0]):\n",
    "            y_batch = self.y[batch]\n",
    "            num_cells = np.amax(y_batch)\n",
    "            for cell in range(1, num_cells + 1):\n",
    "                # count number of pixels cell occupies in each frame\n",
    "                y_true = np.sum(y_batch == cell, axis=(self.row_axis - 1, self.col_axis - 1))\n",
    "                # get indices of frames where cell is present\n",
    "                y_index = np.where(y_true > 0)[0]\n",
    "                if y_index.size > self.min_track_length+1:  # if cell is present at all\n",
    "                    if self.daughters is not None:\n",
    "                        # Only include daughters if there are enough frames in their tracks\n",
    "                        daughter_ids = self.daughters[batch][cell]\n",
    "                        if len(daughter_ids) > 0:\n",
    "                            daughter_track_lengths = []\n",
    "                            for did in daughter_ids:\n",
    "                                d_true = np.sum(y_batch == did, axis=(self.row_axis - 1, self.col_axis - 1))\n",
    "                                d_track_length = len(np.where(d_true>0)[0])\n",
    "                                daughter_track_lengths.append(d_track_length > self.min_track_length+1)\n",
    "                            keep_daughters = all(daughter_track_lengths)\n",
    "                            daughters = daughter_ids if keep_daughters else []\n",
    "                        else:\n",
    "                            daughters = []\n",
    "                    else:\n",
    "                        daughters = []\n",
    "                            \n",
    "                    track_ids[track_counter] = {\n",
    "                        'batch': batch,\n",
    "                        'label': cell,\n",
    "                        'frames': y_index,\n",
    "                        'daughters': daughters  # not [cell-1]!\n",
    "                    }\n",
    "                    track_counter += 1\n",
    "                    \n",
    "                else:\n",
    "                    y_batch[y_batch == cell] = 0\n",
    "                    self.y[batch] = y_batch\n",
    "                    \n",
    "        # Add a field to the track_ids dict that locates all of the different cells\n",
    "        # in each frame\n",
    "\n",
    "        for track in track_ids.keys():\n",
    "            track_ids[track]['different'] = {}\n",
    "            batch = track_ids[track]['batch']\n",
    "            label = track_ids[track]['label']\n",
    "            for frame in track_ids[track]['frames']:\n",
    "                y_unique = np.unique(self.y[batch][frame])\n",
    "                y_unique = np.delete(y_unique, np.where(y_unique == 0))\n",
    "                y_unique = np.delete(y_unique, np.where(y_unique == label))\n",
    "                track_ids[track]['different'][frame] = y_unique  \n",
    "                        \n",
    "        # We will need to look up the track_ids of cells if we know their batch and label. We will \n",
    "        # create a dictionary that stores this information\n",
    "        reverse_track_ids = {}\n",
    "        for batch in range(self.y.shape[0]):\n",
    "            reverse_track_ids[batch] = {}\n",
    "        for track in track_ids.keys():\n",
    "            batch = track_ids[track]['batch']\n",
    "            label = track_ids[track]['label']\n",
    "            reverse_track_ids[batch][label] = track\n",
    "        self.track_ids = track_ids\n",
    "        self.reverse_track_ids = reverse_track_ids\n",
    "        return None\n",
    "\n",
    "    def _get_appearances(self, X, y, frames, labels):\n",
    "        channel_axis = self.channel_axis - 1\n",
    "        if self.data_format == 'channels_first':\n",
    "            appearance_shape = (X.shape[channel_axis],\n",
    "                                len(frames),\n",
    "                                self.crop_dim,\n",
    "                                self.crop_dim)\n",
    "        else:\n",
    "            appearance_shape = (len(frames),\n",
    "                                self.crop_dim,\n",
    "                                self.crop_dim,\n",
    "                                X.shape[channel_axis])\n",
    "\n",
    "        # Initialize storage for appearances and centroids\n",
    "        appearances = np.zeros(appearance_shape, dtype=K.floatx())\n",
    "        centroids = []\n",
    "\n",
    "        for counter, (frame, cell_label) in enumerate(zip(frames, labels)):\n",
    "            # Get the bounding box\n",
    "            y_frame = y[frame] if self.data_format == 'channels_last' else y[:, frame]\n",
    "            props = regionprops(np.int32(y_frame == cell_label))\n",
    "            minr, minc, maxr, maxc = props[0].bbox\n",
    "            centroids.append(props[0].centroid)\n",
    "\n",
    "            # Extract images from bounding boxes\n",
    "            if self.data_format == 'channels_first':\n",
    "                appearance = X[:, frame, minr:maxr, minc:maxc]\n",
    "                resize_shape = (X.shape[channel_axis], self.crop_dim, self.crop_dim)\n",
    "            else:\n",
    "                appearance = X[frame, minr:maxr, minc:maxc, :]\n",
    "                resize_shape = (self.crop_dim, self.crop_dim, X.shape[channel_axis])\n",
    "\n",
    "            # Resize images from bounding box\n",
    "            max_value = np.amax([np.amax(appearance), np.absolute(np.amin(appearance))])\n",
    "            appearance /= max_value\n",
    "            appearance = resize(appearance, resize_shape)\n",
    "            appearance *= max_value\n",
    "            if self.data_format == 'channels_first':\n",
    "                appearances[:, counter] = appearance\n",
    "            else:\n",
    "                appearances[counter] = appearance\n",
    "\n",
    "        return [appearances, centroids]\n",
    "\n",
    "    def _create_appearances(self):\n",
    "        \"\"\"\n",
    "        This function gets the appearances of every cell, crops them out, resizes them, \n",
    "        and stores them in an matrix. Pre-fetching the appearances should significantly \n",
    "        speed up the generator\n",
    "        \"\"\"\n",
    "        number_of_tracks = len(self.track_ids.keys())\n",
    "\n",
    "        # Initialize the array for the appearances and centroids\n",
    "        if self.data_format =='channels_first':\n",
    "            all_appearances_shape = (number_of_tracks, self.x.shape[self.channel_axis], self.x.shape[self.time_axis], self.crop_dim, self.crop_dim)\n",
    "        if self.data_format == 'channels_last':\n",
    "            all_appearances_shape = (number_of_tracks, self.x.shape[self.time_axis], self.crop_dim, self.crop_dim, self.x.shape[self.channel_axis])\n",
    "        all_appearances = np.zeros(all_appearances_shape, dtype = K.floatx())\n",
    "\n",
    "        all_centroids_shape = (number_of_tracks, self.x.shape[self.time_axis],2)\n",
    "        all_centroids = np.zeros(all_centroids_shape, dtype = K.floatx())\n",
    "\n",
    "        for track in self.track_ids.keys():\n",
    "            batch = self.track_ids[track]['batch']\n",
    "            label = self.track_ids[track]['label']\n",
    "            frames = self.track_ids[track]['frames']\n",
    "\n",
    "            # Make an array of labels that the same length as the frames array\n",
    "            labels = [label] * len(frames)\n",
    "            X = self.x[batch]\n",
    "            y = self.y[batch]\n",
    "\n",
    "            app = self._get_appearances(X, y, frames, labels)\n",
    "            appearance = app[0]\n",
    "            centroid = app[1]\n",
    "\n",
    "            if self.data_format == 'channels_first':\n",
    "                all_appearances[track,:,np.array(frames),:,:] = appearance \n",
    "            if self.data_format == 'channels_last':\n",
    "                all_appearances[track,np.array(frames),:,:,:] = appearance\n",
    "\n",
    "            all_centroids[track,np.array(frames),:] = centroid\n",
    "\n",
    "        self.all_appearances = all_appearances\n",
    "        self.all_centroids = all_centroids\n",
    "\n",
    "        return None\n",
    "\n",
    "    def _fetch_appearances(self, track, frames):\n",
    "        \"\"\"\n",
    "        This function gets the appearances after they have been \n",
    "        cropped out of the image\n",
    "        \"\"\"\n",
    "        # TO DO: Check to make sure the frames are acceptable\n",
    "\n",
    "        if self.data_format == 'channels_first':\n",
    "            appearances = self.all_appearances[track,:,np.array(frames),:,:]\n",
    "        if self.data_format == 'channels_last':\n",
    "            appearances = self.all_appearances[track,np.array(frames),:,:,:]\n",
    "        return appearances\n",
    "\n",
    "    def _fetch_centroids(self, track, frames):\n",
    "        \"\"\"\n",
    "        This function gets the centroids after they have been\n",
    "        extracted and stored\n",
    "        \"\"\"\n",
    "        # TO DO: Check to make sure the frames are acceptable\n",
    "        \n",
    "        centroids = self.all_centroids[track,np.array(frames),:]\n",
    "        return centroids\n",
    "\n",
    "    def _fetch_frames(self, track, division=False):\n",
    "        \"\"\"\n",
    "        This function fetches a random list of frames for a given track.\n",
    "        If the division flag is true, then the list of frames ends at the cell's\n",
    "        last appearance if the division flag is true.\n",
    "        \"\"\"\n",
    "        track_id = self.track_ids[track]\n",
    "        batch = track_id['batch']\n",
    "        tracked_frames = list(track_id['frames'])\n",
    "\n",
    "        # We need to have at least one future frame to pick from, so if \n",
    "        # the last frame of the movie is a tracked frame, remove it\n",
    "        last_frame = self.x.shape[self.time_axis] - 1\n",
    "        if last_frame in tracked_frames:\n",
    "            tracked_frames.remove(last_frame)\n",
    "\n",
    "        # Get the indices of the tracked_frames list - sometimes frames\n",
    "        # are skipped\n",
    "        tracked_frames_index = np.arange(len(tracked_frames))\n",
    "        \n",
    "        # Check if there are enough frames\n",
    "        enough_frames = len(tracked_frames_index) > self.min_track_length + 1\n",
    "\n",
    "        # We need to exclude the last frame so that we will always be able to make a comparison\n",
    "        acceptable_indices = tracked_frames_index[self.min_track_length-1:-1] if enough_frames else tracked_frames_index[:-1]\n",
    "\n",
    "        # Take the last frame if there is a division, otherwise randomly pick a frame\n",
    "#         print(tracked_frames_index)\n",
    "#         print(division)\n",
    "#         print(acceptable_indices)\n",
    "        index = -1 if division else np.random.choice(acceptable_indices) \n",
    "\n",
    "        # Select the frames. If there aren't enough frames, repeat the first frame\n",
    "        # the necessary number of times\n",
    "        if enough_frames:\n",
    "            frames = tracked_frames[index+1-self.min_track_length:index+1]\n",
    "        else:\n",
    "            frames_temp = tracked_frames[0:index+1]\n",
    "            missing_frames = self.min_track_length - len(frames_temp)\n",
    "            frames = tracked_frames[0] * missing_frames + frames_temp\n",
    "\n",
    "        return frames\n",
    "\n",
    "    def _get_batches_of_transformed_samples(self, index_array):\n",
    "        # Initialize batch_x_1, batch_x_2, and batch_y, as well as cell distance data\n",
    "        # DVV Notes - I'm changing how this works. We will now only compare cells in neighboring\n",
    "        # frames. I am also modifying it so it will select a sequence of cells/distances for x1\n",
    "        # and 1 cell/distance for x2\n",
    "\n",
    "        if self.data_format == 'channels_first':\n",
    "            x1_shape = (len(index_array), self.x.shape[self.channel_axis], self.min_track_length, self.crop_dim, self.crop_dim)\n",
    "            x2_shape = (len(index_array), self.x.shape[self.channel_axis], self.crop_dim, self.crop_dim)\n",
    "        else:\n",
    "            x1_shape = (len(index_array), self.min_track_length, self.crop_dim, self.crop_dim, self.x.shape[self.channel_axis])\n",
    "            x2_shape = (len(index_array), 1, self.crop_dim, self.crop_dim, self.x.shape[self.channel_axis])\n",
    "        distance_shape_1 = (len(index_array), self.min_track_length, 2)\n",
    "        distance_shape_2 = (len(index_array), 1, 2)\n",
    "        y_shape = (len(index_array), 3)\n",
    "\n",
    "        batch_x_1 = np.zeros(x1_shape, dtype=K.floatx())\n",
    "        batch_x_2 = np.zeros(x2_shape, dtype=K.floatx())\n",
    "        batch_distance_1 = np.zeros(distance_shape_1, dtype=K.floatx())\n",
    "        batch_distance_2 = np.zeros(distance_shape_2, dtype=K.floatx())\n",
    "        batch_y = np.zeros(y_shape, dtype=np.int32)\n",
    "\n",
    "        for i, j in enumerate(index_array):\n",
    "            # Identify which tracks are going to be selected\n",
    "            track_id = self.track_ids[j]\n",
    "            batch = track_id['batch']\n",
    "            label_1 = track_id['label']  \n",
    "            \n",
    "            X = self.x[batch]\n",
    "            y = self.y[batch]\n",
    "\n",
    "            # Choose comparison cell\n",
    "            # Determine what class the track will be - different (0), same (1), division (2)\n",
    "            # is_same_cell = np.random.random_integers(0, 1)\n",
    "            division = False\n",
    "            type_cell = np.random.randint(0, 3)\n",
    "\n",
    "            # Dealing with edge cases\n",
    "\n",
    "            # If class is division, check if the first cell divides. If not, change class to same/dif randomly\n",
    "            if type_cell == 2:\n",
    "                daughters = track_id['daughters']\n",
    "                if len(daughters) == 0:\n",
    "                    type_cell = np.random.random_integers(0, 1)  # No children so randomly choose a diff class\n",
    "                else:\n",
    "                    division == True\n",
    "                    \n",
    "            # Get the frames for cell 1 and frames/label for cell 2\n",
    "            frames_1 = self._fetch_frames(j, division=division)\n",
    "            \n",
    "            # For frame_2, choose the next frame cell 1 appears in \n",
    "            last_frame_1 = np.amax(frames_1)\n",
    "            frame_2 = np.amin( [x for x in track_id['frames'] if x > last_frame_1] )\n",
    "            frames_2 = [frame_2]\n",
    "\n",
    "            different_cells = track_id['different'][frame_2]\n",
    "                    \n",
    "            if type_cell == 0:\n",
    "                # If there are no different cells in the subsequent frame, we must choose \n",
    "                # the same cell\n",
    "                if len(different_cells) == 0:\n",
    "                    type_cell = 1\n",
    "                else:\n",
    "                    label_2 = np.random.choice(different_cells)\n",
    "\n",
    "            if type_cell == 1:\n",
    "                # If there is only 1 cell in frame_2, we can only choose the class to be same\n",
    "                label_2 = label_1\n",
    "                        \n",
    "            if type_cell == 2:\n",
    "                # There should always be 2 daughters but not always a valid label\n",
    "                label_2 = int(daughters[np.random.random_integers(0, len(daughters)-1)])\n",
    "\n",
    "            track_1 = j\n",
    "            track_2 = self.reverse_track_ids[batch][label_2]\n",
    "\n",
    "            # Get appearances and centroid data\n",
    "            appearance_1 = self._fetch_appearances(track_1, frames_1)\n",
    "            appearance_2 = self._fetch_appearances(track_2, frames_2)\n",
    "            centroid_1 = self._fetch_centroids(track_1, frames_1)\n",
    "            centroid_2 = self._fetch_centroids(track_2, frames_2)\n",
    "\n",
    "            # Apply random transforms\n",
    "            new_appearance_1 = np.zeros(appearance_1.shape, dtype = K.floatx())\n",
    "            new_appearance_2 = np.zeros(appearance_2.shape, dtype = K.floatx())\n",
    "\n",
    "            for frame in range(appearance_1.shape[self.time_axis-1]):\n",
    "                if self.data_format == 'channels_first':\n",
    "                    app_temp = self.image_data_generator.random_transform(appearance_1[:,frame,:,:])\n",
    "                    app_temp = self.image_data_generator.standardize(app_temp)\n",
    "                    new_appearance_1[:,frame,:,:] = app_temp                        \n",
    "\n",
    "                if self.data_format == 'channels_last':\n",
    "                    app_temp = self.image_data_generator.random_transform(appearance_1[frame])\n",
    "                    app_temp = self.image_data_generator.standardize(app_temp)\n",
    "                    new_appearance_1[frame] = app_temp\n",
    "\n",
    "            if self.data_format == 'channels_first':\n",
    "                app_temp = self.image_data_generator.random_transform(appearance_2[:,0,:,:])\n",
    "                app_temp = self.image_data_generator.standardize(app_temp)\n",
    "                new_appearance_2[:,0,:,:] = app_temp   \n",
    "                \n",
    "            if self.data_format == 'channels_last':\n",
    "                app_temp = self.image_data_generator.random_transform(appearance_2[0])\n",
    "                app_temp = self.image_data_generator.standardize(app_temp)\n",
    "                new_appearance_2[0] = app_temp\n",
    "\n",
    "            # Compute distances between centroids\n",
    "            centroids = np.concatenate([centroid_1, centroid_2], axis=0)\n",
    "            distance = np.diff(centroids, axis=0)\n",
    "            zero_pad = np.zeros((1,2), dtype = K.floatx())\n",
    "            distance = np.concatenate([zero_pad, distance], axis=0)\n",
    "\n",
    "            # Save images and distances to the batch arrays\n",
    "            batch_x_1[i] = appearance_1\n",
    "            batch_x_2[i] = appearance_2\n",
    "            batch_distance_1[i] = distance[0:-1,:]\n",
    "            batch_distance_2[i,0,:] = distance[-1,:]\n",
    "            batch_y[i, type_cell] = 1\n",
    "   \n",
    "        # Remove singleton dimensions (if min_track_length is 1)\n",
    "        if self.squeeze:\n",
    "            batch_x_1 = np.squeeze(batch_x_1, axis=self.time_axis)\n",
    "            batch_x_2 = np.squeeze(batch_x_2, axis=self.time_axis)\n",
    "            batch_distance_1 = np.squeeze(batch_distance_1, axis=1)\n",
    "            batch_distance_2 = np.squeeze(batch_distance_2, axis=1)\n",
    "        return [batch_x_1, batch_x_2, batch_distance_1, batch_distance_2], batch_y\n",
    "\n",
    "    def next(self):\n",
    "        \"\"\"For python 2.x.\n",
    "        # Returns the next batch.\n",
    "        \"\"\"\n",
    "        # Keeps under lock only the mechanism which advances\n",
    "        # the indexing of each batch.\n",
    "        with self.lock:\n",
    "            index_array = next(self.index_generator)\n",
    "        # The transformation of images is not under thread lock\n",
    "        # so it can be done in parallel\n",
    "        return self._get_batches_of_transformed_samples(index_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def siamese_model(input_shape=None, track_length=1, batch_shape=None, reg=1e-5, init='he_normal', softmax=True, norm_method='std', filter_size=61):\n",
    "\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        channel_axis = 1\n",
    "        new_input_shape = tuple([input_shape[0]] + [None] + list(input_shape[1:]))\n",
    "    else:\n",
    "        channel_axis = -1\n",
    "        new_input_shape = tuple([None] + list(input_shape))\n",
    "        \n",
    "    input_shape = new_input_shape\n",
    "\n",
    "    # Define the input shape for the images\n",
    "    input_1 = Input(shape=input_shape)\n",
    "    input_2 = Input(shape=input_shape)\n",
    "    # Define the input shape for the other data (centroids, etc)\n",
    "    input_3 = Input(shape=(None, 2))\n",
    "    input_4 = Input(shape=(None, 2))\n",
    "\n",
    "    # Sequential interface for siamese portion of model\n",
    "    feature_extractor = Sequential()\n",
    "    feature_extractor.add(Conv3D(64, (1, 3, 3), kernel_initializer=init, padding='same', kernel_regularizer=l2(reg), input_shape=input_shape))\n",
    "    feature_extractor.add(BatchNormalization(axis=channel_axis))\n",
    "    feature_extractor.add(Activation('relu'))\n",
    "    feature_extractor.add(MaxPool3D(pool_size=(1, 2, 2)))\n",
    "    \n",
    "    feature_extractor.add(Conv3D(64, (1, 3, 3), kernel_initializer=init, padding='same', kernel_regularizer=l2(reg)))\n",
    "    feature_extractor.add(BatchNormalization(axis=channel_axis))\n",
    "    feature_extractor.add(Activation('relu'))\n",
    "    feature_extractor.add(MaxPool3D(pool_size=(1, 2, 2)))\n",
    "    \n",
    "    feature_extractor.add(Conv3D(64, (1, 3, 3), kernel_initializer=init, padding='same', kernel_regularizer=l2(reg)))\n",
    "    feature_extractor.add(BatchNormalization(axis=channel_axis))\n",
    "    feature_extractor.add(Activation('relu'))\n",
    "    feature_extractor.add(MaxPool3D(pool_size=(1, 2, 2)))\n",
    "\n",
    "    feature_extractor.add(Conv3D(64, (1, 3, 3), kernel_initializer=init, padding='same', kernel_regularizer=l2(reg)))\n",
    "    feature_extractor.add(BatchNormalization(axis=channel_axis))\n",
    "    feature_extractor.add(Activation('relu'))\n",
    "    feature_extractor.add(MaxPool3D(pool_size=(1, 2, 2)))\n",
    "    \n",
    "    feature_extractor.add(Conv3D(64, (1, 3, 3), kernel_initializer=init, padding='same', kernel_regularizer=l2(reg)))\n",
    "    feature_extractor.add(BatchNormalization(axis=channel_axis))\n",
    "    feature_extractor.add(Activation('relu'))\n",
    "    feature_extractor.add(MaxPool3D(pool_size=(1, 2, 2)))\n",
    "    \n",
    "    feature_extractor.add(Reshape(tuple([64])))\n",
    "\n",
    "    # Create two instances of feature_extractor\n",
    "    output_1 = feature_extractor(input_1)\n",
    "    output_2 = feature_extractor(input_2)\n",
    "\n",
    "    input_3_reshape = Reshape((2,))(input_3)\n",
    "    input_4_reshape = Reshape((2,))(input_4)\n",
    "\n",
    "    # Combine the extracted features with other known features (centroids)\n",
    "    merge_1 = Concatenate(axis=channel_axis)([output_1, output_2])\n",
    "    merge_2 = Concatenate(axis=channel_axis)([input_3_reshape, input_4_reshape])\n",
    "\n",
    "    # Concatenate outputs from both instances\n",
    "    merged_outputs = Concatenate(axis=channel_axis)([merge_1, merge_2])\n",
    "\n",
    "    # Implement dense net (Alternatively, could call preexisting) with the 2 merged outputs as inputs\n",
    "    dense1 = Dense(128)(merged_outputs)\n",
    "    bn1 = BatchNormalization(axis=channel_axis)(dense1)\n",
    "    relu1 = Activation('relu')(bn1)\n",
    "    dense2 = Dense(128)(relu1)\n",
    "    bn2 = BatchNormalization(axis=channel_axis)(dense2)\n",
    "    relu2 = Activation('relu')(bn2)\n",
    "    dense3 = Dense(3, activation='softmax')(relu2)\n",
    "\n",
    "    # Instantiate model\n",
    "    final_layer = dense3\n",
    "    model = Model(inputs=[input_1, input_2, input_3, input_4], outputs=final_layer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs(X, y, frames, labels, crop_dim=14, data_format=\"channels_last\"):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def cost_matrix(X, y): # X = data['X'], y = data['y']...a single movie\n",
    "    for frame in range(X.shape[1]-1): # for each frame (pair of) in the movie\n",
    "\n",
    "        # get pair comparisons for the pair of frames\n",
    "        annotation = y[segment][frame+1] # the label of the second frame is original annotation from deepcell\n",
    "        labels_in_second = np.uint8(np.unique(y)) \n",
    "        labels_in_second = np.delete(labels_in_second, np.where(labels_in_second==0))\n",
    "        labels_in_first = np.uint8(np.unique(new_annotations[frame])) \n",
    "        labels_in_first = np.delete(labels_in_first, np.where(labels_in_first==0))\n",
    "        total_cells = len(labels_in_first) + len(labels_in_second)\n",
    "        cost_matrix = np.zeros(shape=(1, total_cells, total_cells), dtype=np.float64)\n",
    "        \n",
    "        compairs = []\n",
    "        for s_label in labels_in_second:\n",
    "            for f_label in labels_in_first:\n",
    "                compairs.append([f_label, s_label])\n",
    "        print(compairs)\n",
    "\n",
    "        # get batches of inputs to model, data generator functions\n",
    "        # predict, save outputs\n",
    "        # use outputs to calculate area 1, 2, 3, set in place in cost matrix\n",
    "        # linear assignment\n",
    "        # new annotations!\n",
    "        # lineage (confirm to use area 2 & 3, but need to use new labels)\n",
    "            # since (theoretically) the parent should die, \n",
    "            # access only the output of dead cells for a new cell to determine division\n",
    "            \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
